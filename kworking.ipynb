{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire (`acquire.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zillow\n",
    "**For the following, iterate through the steps you would take to create functions: Write the code to do the following in a jupyter notebook, test it, convert to functions, then create the file to house those functions.**\n",
    "\n",
    "**You will have a** \n",
    "`zillow.ipynb` \n",
    "**file and a helper file for each section in the pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "### python imports                                                          ###\n",
    "###############################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# from math import sqrt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "### local imports                                                           ###\n",
    "###############################################################################\n",
    "\n",
    "import acquire as acq\n",
    "import prepare as prep\n",
    "\n",
    "from debug import local_settings, timeifdebug, timeargsifdebug, frame_splain\n",
    "from dfo import DFO, set_dfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_settings.splain=True\n",
    "local_settings.debug=True\n",
    "splain = local_settings.splain\n",
    "debug = local_settings.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrangle_zillow import get_zillow_data, prep_zillow_data, sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire & Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Acquire data from mySQL using the python module to connect and query. You will want to end with a single dataframe. Make sure to include: the logerror, all fields related to the properties that are available. You will end up using all the tables in the database.**\n",
    "\n",
    "   - *Be sure to do the correct join (inner, outer, etc.). We do not want to eliminate properties purely because they may have a null value for* \n",
    "`airconditioningtypeid`\n",
    "*.*\n",
    "   - Only include properties with a transaction in 2017, and include only the last transaction for each properity (so no duplicate property ID's), along with zestimate error and date of transaction.\n",
    "   - Only include properties that include a latitude and longitude value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 10:29:31 starting get_zillow_data\n",
      "2019-11-06 10:29:31 starting sql_df\n",
      "2019-11-06 10:29:31 starting get_db_url\n",
      "2019-11-06 10:29:31 ending get_db_url ; time: 0:00:00.000086\n"
     ]
    }
   ],
   "source": [
    "dfo = set_dfo(get_zillow_data(), splain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Summarize your data (summary stats, info, dtypes, shape, distributions, value_counts, etc.).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfo.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_splain(dfo.df, title='Zillow Data', splain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeifdebug\n",
    "def convert_to_dates(df, cols=[], format='%Y-%m-%d', errors='coerce', **kwargs):\n",
    "    '''\n",
    "    convert_to_dates(\n",
    "        df, \n",
    "        cols, \n",
    "        format='%y-%m-%d', \n",
    "        errors='coerce', \n",
    "        **kwargs\n",
    "    )\n",
    "    RETURNS df with columns updated to dates\n",
    "    '''\n",
    "    convertcols = [col for col in cols if col in df.columns]\n",
    "    for col in convertcols:\n",
    "        df[col] = pd.to_datetime(df[col], errors=errors, format=format)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_to_dates(df, cols=['transactiondate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo.df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_splain(dfo.df, splain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: **Write a function that takes in a dataframe of observations and attributes and returns a dataframe where each row is an atttribute name, the first column is the number of rows with missing values for that attribute, and the second column is percent of total rows that have missing values for that attribute. Run the function and document takeaways from this on how you want to handle missing values.**\n",
    "\n",
    "||num_rows_missing|pct_rows_missing|\n",
    "|:---|---:|---:|\n",
    "|parcelid|0|0.000000|\n",
    "|airconditioningtypeid|29041|0.535486|\n",
    "|architecturalstyletypeid|54232|0.999982|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeifdebug\n",
    "def get_column_values_stats(\n",
    "        df, \n",
    "        get_cols=[], \n",
    "        max_uniques=10, \n",
    "        target_col='',\n",
    "        limit_to_max=False,\n",
    "        **kwargs\n",
    "        ):\n",
    "    '''\n",
    "    get_column_values_stats(\n",
    "        df, \n",
    "        get_cols=[], \n",
    "        max_uniques=10, \n",
    "        target_col='', \n",
    "        limit_to_max=False,\n",
    "        **kwargs\n",
    "        )\n",
    "    RETURNS summary dataframe\n",
    "\n",
    "    Receives dataframe as input, examines all columns defined as objects, and\n",
    "    returns a summary report with column name as its index.\n",
    "\n",
    "    Report shows the following information about the columns of the dataframe:\n",
    "        cols: column name\n",
    "        dtypes: data type\n",
    "        num_rows_values: number of rows with non-null values \n",
    "        num_rows_missing: number of rows with missing values\n",
    "        pct_rows_missing: percentage of rows with missing values\n",
    "        num_uniques: number of unique values \n",
    "        unique_values: list of unique values if the unique value count is less than \n",
    "            or equal to max_uniques.\n",
    "        \n",
    "    If the input dataframe contains the target column, enter that name as the \n",
    "    target_col argument and it will be removed from the analysis.\n",
    "    \n",
    "    If limit_to_max is True (default), the resulting dataframe will only show\n",
    "    columns with no more unique values than specified in max_uniques.\n",
    "    \n",
    "    NOTE: This function can be used to identify and then re-type catergorical\n",
    "    columns. Run the function with the following parameters, change the data \n",
    "    types of know categorical values to 'category', then re-run this function, \n",
    "    gradually increasing the max_uniques value until the desired limit for \n",
    "    categoricals has been met.\n",
    "    \n",
    "    get_column_values_stats(\n",
    "        df, \n",
    "        get_cols=df.columns[df.dtypes!='category'],\n",
    "        max_uniques=5,\n",
    "        limit_to_max=True\n",
    "        )\n",
    "    '''\n",
    "    # get rowcount\n",
    "    num_recs = len(df)\n",
    "    \n",
    "    # if no columns presented, get all columns from df\n",
    "    use_cols = df.columns if len(get_cols) == 0 else get_cols\n",
    "    # ensure use_cols are actually present in df\n",
    "    cols = [col for col in use_cols if col in df.columns]\n",
    "    \n",
    "    # make df for all columns, exclude target column if passed\n",
    "    df_cols = pd.DataFrame(cols, columns=['cols'])\n",
    "    df_cols = df_cols[df_cols.cols != target_col]\n",
    "    \n",
    "    # get statistics on all columns\n",
    "    df_cols['dtype'] = df_cols.cols.apply(lambda x: df[x].dtype)\n",
    "    df_cols['num_rows_values'] = df_cols.cols.apply(lambda x: df[x].count())\n",
    "    df_cols['num_rows_missing'] = num_recs - df_cols.num_rows_values\n",
    "    df_cols['pct_rows_missing'] = df_cols.num_rows_missing / num_recs\n",
    "    df_cols['num_uniques'] = df_cols.cols.apply(lambda x: df[x].nunique())\n",
    "    \n",
    "    # get unique valeues for columns within unique value limits\n",
    "    df_cats = df_cols[df_cols.num_uniques <= max_uniques]\n",
    "    df_cats['unique_values'] = df_cats.cols.apply(lambda x: df[x].unique())\n",
    "    \n",
    "    # merge and set index\n",
    "    join_type = 'inner' if limit_to_max else 'left'\n",
    "    df_cols = df_cols.join(df_cats.unique_values, how=join_type)\n",
    "    return df_cols.set_index('cols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find string categoricals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strings = get_column_values_stats(\n",
    "    df, \n",
    "    get_cols=df.columns[df.dtypes=='object'],\n",
    "    max_uniques=20000,\n",
    "    limit_to_max=True\n",
    ")\n",
    "df_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strings.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_categoricals = [\n",
    "    'airconditioningdesc', \n",
    "    'architecturalstyledesc', \n",
    "    'buildingclassdesc', \n",
    "    'county', \n",
    "    'state', \n",
    "    'heatingorsystemdesc', \n",
    "    'propertycountylandusecode', \n",
    "    'propertylandusedesc', \n",
    "    'propertyzoningdesc', \n",
    "    'storydesc', \n",
    "    'taxdelinquencyflag', \n",
    "    'typeconstructiondesc'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep.retype_cols(df, cols=string_categoricals, to_dtype='category')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find Numeric Categoricals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_check = get_column_values_stats(\n",
    "    df, \n",
    "    get_cols=df.columns[df.dtypes!='category'],\n",
    "    max_uniques=750000,\n",
    "    limit_to_max=True\n",
    ")\n",
    "df_cat_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_check.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_categoricals = [\n",
    "    'airconditioningtypeid', \n",
    "    'architecturalstyletypeid', \n",
    "    'buildingclasstypeid', \n",
    "    'buildingqualitytypeid',\n",
    "    'decktypeid', \n",
    "    'fips', \n",
    "    'hashottuborspa',\n",
    "    'heatingorsystemtypeid',\n",
    "    'pooltypeid10', \n",
    "    'pooltypeid2', \n",
    "    'pooltypeid7', \n",
    "    'propertylandusetypeid',\n",
    "    'regionidcounty', \n",
    "    'regionidcity',\n",
    "    'regionidneighborhood',\n",
    "    'regionidzip',\n",
    "    'rawcensustractandblock',\n",
    "    'censustractandblock',\n",
    "    'storytypeid', \n",
    "    'threequarterbathnbr', \n",
    "    'numberofstories', \n",
    "    'fireplaceflag', \n",
    "    'assessmentyear', \n",
    "    'typeconstructiontypeid', \n",
    "    'transactions'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep.retype_cols(df, cols=numeric_categoricals, to_dtype='category')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Write a function that takes in a dataframe and returns a dataframe with 3 columns: the number of columns missing, percent of columns missing, and number of rows with n columns missing. Run the function and document takeaways from this on how you want to handle missing values.**\n",
    "\n",
    "num_cols_missing|pct_cols_missing|num_rows\n",
    ":---|---:|---:\n",
    "23|38.333|108\n",
    "24|40.000|123\n",
    "25|41.667|5280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nulls_by_row(df):\n",
    "    total_cols = df.shape[1]\n",
    "    num_cols_missing = df.isnull().sum(axis=1)\n",
    "    pct_cols_missing = df.isnull().sum(axis=1)/ total_cols\n",
    "    rows_missing = (\n",
    "        pd.DataFrame(\n",
    "        {\n",
    "            'num_cols_missing': num_cols_missing, \n",
    "            'pct_cols_missing': pct_cols_missing\n",
    "        }\n",
    "        )\n",
    "        .reset_index()\n",
    "        .groupby(['num_cols_missing','pct_cols_missing'])\n",
    "        .count()\n",
    "        .rename(index=str, columns={'index': 'num_rows'})\n",
    "        .reset_index()\n",
    "    )\n",
    "    return rows_missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_by_row(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove any properties that are likely to be something other than single unit properties. (e.g. no duplexes, no land/lot, ...). There are multiple ways to estimate that a property is a single unit, and there is not a single \"right\" answer. But for this exercise, do not purely filter by unitcnt as we did previously. Add some new logic that will reduce the number of properties that are falsely removed. You might want to use # bedrooms, square feet, unit type or the like to then identify those with unitcnt not defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a function that will drop rows or columns based on the percent of values that are missing: `handle_missing_values(df, prop_required_column, prop_required_row)`.\n",
    "\n",
    "   - The input:\n",
    "       - A dataframe\n",
    "       - A number between 0 and 1 that represents the proportion, for each column, of rows with non-missing values required to keep the column. i.e. if prop_required_column = .6, then you are requiring a column to have at least 60% of values not-NA (no more than 40% missing).\n",
    "       - A number between 0 and 1 that represents the proportion, for each row, of columns/variables with non-missing values required to keep the row. For example, if prop_required_row = .75, then you are requiring a row to have at least 75% of variables with a non-missing value (no more that 25% missing).\n",
    "   - The output:\n",
    "       - The dataframe with the columns and rows dropped as indicated. Be sure to drop the columns prior to the rows in your function.\n",
    "   - hint:\n",
    "       - Look up the dropna documentation.\n",
    "       - You will want to compute a threshold from your input values (prop_required) and total number of rows or columns.\n",
    "       - Make use of inplace, i.e. inplace=True/False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(\n",
    "    df, \n",
    "    prop_required_column, \n",
    "    prop_required_row\n",
    "    ):\n",
    "    '''\n",
    "    handle_missing_values(\n",
    "    df, \n",
    "    prop_required_column=.9, \n",
    "    prop_required_row=.9\n",
    "    )\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Decide how to handle the remaining missing values:\n",
    "   - Fill with constant value.\n",
    "   - Impute with mean, median, mode.\n",
    "   - Drop row/column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sqft = df[['calculatedfinishedsquarefeet',\n",
    "'finishedfloor1squarefeet',\n",
    "'finishedsquarefeet6',\n",
    "'finishedsquarefeet12',\n",
    "'finishedsquarefeet13',\n",
    "'finishedsquarefeet15',\n",
    "'finishedsquarefeet50'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `wrangle_zillow.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions of the work above needed to acquire and prepare a new sample of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sqft[df_sqft.calculatedfinishedsquarefeet.isna() & (df_sqft.finishedsquarefeet50.isna()==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool = df[['poolcnt','poolsizesum','pooltypeid2','pooltypeid7','pooltypeid10']]\n",
    "df_pool[(df_pool.poolcnt.isna()==False) | (df_pool.pooltypeid10.isna()==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
